{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e9a94b",
   "metadata": {},
   "source": [
    "#  PREPARING TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea406bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_set = []\n",
    "labels = dict(enumerate([])  ## we have to fill the labels as given in the folder containing the dataset\n",
    "TRAIN_DIR = 'D:\\Emoji_dataset/' # the directory of the training images.. (here it is an example)\n",
    "\n",
    "def training_data(i):\n",
    "        count = 0\n",
    "        for img in tqdm(os.listdir(TRAIN_DIR + labels[i])):   # img here is just a iterating parameter...\n",
    "            path = os.path.join(TRAIN_DIR + labels[i], img)  ## path joining to the image..\n",
    "            image = cv2.imread(path)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            ret, thersh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            training_set.append([np.array(thersh), str(i)])\n",
    "            count+=1\n",
    "            if count > 4500:   # to train 4500 images from each folders..\n",
    "                #print(thersh)  this thersh is image in array in pixel form...\n",
    "                break\n",
    "        \n",
    "for i in range(len(labels)):\n",
    "    training_data(i)\n",
    "    \n",
    "np.random.shuffle(training_set)\n",
    "np.save('training_set_part_2.npy', training_set)\n",
    "np.load('training_set_part_2.npy', allow_pickle = True)\n",
    "print('\\n')\n",
    "print('SUCCESS') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abbe78",
   "metadata": {},
   "source": [
    "# Here, we are preparing test, validation and training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('training_set_part_2.npy', allow_pickle = True)\n",
    "\n",
    "# splitting test and training set..\n",
    "\n",
    "scale_of_training = 0.6   # using default 60 % to train and 40 % to test or validation\n",
    "\n",
    "# def training_feed(training_data): trying to define a separate function to separate train, validation and test set..\n",
    "\n",
    "'''\n",
    "we can also split the training and validation data under the model.fit but we di here for better data handling\n",
    "and transparency.\n",
    "'''\n",
    "\n",
    "\n",
    "train = training_data[:-int(scale_of_training*len(training_data))]\n",
    "valid = training_data[-int(scale_of_training*len(training_data)): -int((1-scale_of_training)*len(training_data))]\n",
    "test = training_data[-int((1-scale_of_training)*len(training_data)) :]\n",
    "\n",
    "trainX = np.array([train[i][0] for i in range(len(train))])\n",
    "trainY = to_categorial([i[1] for i in train])\n",
    "validX = np.array([valid[i][0] for i in range(len(valid))])\n",
    "validY = to_categorial([i[1] for i in valid])\n",
    "testX = np.array(test[i][0] for i in range(len(test)))\n",
    "testY = to_categorial([i[1] for i in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651ea32",
   "metadata": {},
   "source": [
    "#  Here, we are desiging the final part of model, which I shared before, the following part will make our model functional and able to analyse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(train_X, valid_X, train_Y, valid_Y):\n",
    "#     img_size = train_X.shape()\n",
    "#     ### prepating 1\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(64, (2,2), input_shape = img_size, strides = 2))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())  \n",
    "    \n",
    "#     #  preparing the 2.a of the model..\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(96, (1,1), input_shape = img_size, strides = 1, activation = \"relu\"))\n",
    "#     model.add(Conv2D(208, (3,3), input_shape = img_size, strides = 1, padding = \"same\", activation = \"relu\"))\n",
    "   \n",
    "#     #  preparing the 2.b of the model..\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(MaxPooling2D(pool_size = (1,1), input_shape = img_size))\n",
    "#     model.add(Conv2D(64, (1,1), input_shape = img_size, strides = 1, activation = \"relu\"))\n",
    "    \n",
    "#     # preparing 3.a\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(96, (1,1), input_shape = img_size, strides = 1, activation = \"relu\"))\n",
    "#     model.add(Conv2D(208, (1,1), input_shape = img_size, strides = 1, activation = \"relu\"))\n",
    "    \n",
    "#     # preparing 3.b\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(MaxPooling2D(pool_size = (1,1), input_shape = img_size))\n",
    "#     model.add(Conv2D(64, (1,1), input_shape = img_size, strides = 1, activation = \"relu\"))\n",
    "   \n",
    "#                   metrics=['accuracy'])\n",
    "#     history = model.fit(train_X, train_Y, batch_size=32, epochs=10, verbose=1,\n",
    "#                         validation_data=(valid_X, valid_Y),\n",
    "#                         callbacks=[check_point])\n",
    "    \n",
    "#     # using the optimiser...\n",
    "\n",
    "#     model.compile(optimizer = \"adam\", loss = \"sparse_categorial_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
    "\n",
    "#     ## training on test and validation set\n",
    "\n",
    "#     model_overloading = model.fit(trainX, trainY, validation_data=(validX, validY), batch_size=128, epochs=6)\n",
    "\n",
    "#     # test time !!\n",
    "\n",
    "#     model_test_drive = model.evaluate(testX, testY, batch_size = 64, steps = 2)\n",
    "\n",
    "#     # model_test_drive = [lest_loss, test_accuracy]\n",
    "\n",
    "#     # plotting the test_drive datas :\n",
    "\n",
    "    \n",
    "#     model.summary()\n",
    "#     model.save(\"emoji_prediction.h5\")\n",
    "    \n",
    "#     return history   # this will be feed t oh_list as input.\n",
    "\n",
    "# ###  Now, finally we have to add the classifier.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using the optimiser...\n",
    "\n",
    "# model.compile(optimizer = \"adam\", loss = \"sparse_categorial_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
    "\n",
    "# ## training on test and validation set\n",
    "\n",
    "# model_overloading = model.fit(trainX, trainY, validation_data=(validX, validY), batch_size=128, epochs=6)\n",
    "\n",
    "# # test time !!\n",
    "\n",
    "# model_test_drive = model.evaluate(testX, testY, batch_size = 64, steps = 2)\n",
    "\n",
    "# # model_test_drive = [lest_loss, test_accuracy]\n",
    "\n",
    "# # plotting the test_drive datas :\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
