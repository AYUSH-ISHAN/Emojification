{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaseModel_Resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pretrained Resnet50 model on kaggle preprocessed dataset"
      ],
      "metadata": {
        "id": "fIHC-lYQINEX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEfxQh1zTrsk",
        "outputId": "6670b55c-8226-428d-873b-bbc19f5e53f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import csv\n",
        "from PIL import Image    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D,BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tqdm import tqdm\n",
        "import numpy as np # linear algebra\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import collections\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split \n",
        "import os\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras import layers\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, LSTM, merge\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import csv\n",
        "from PIL import Image    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D,BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tqdm import tqdm\n",
        "import numpy as np # linear algebra\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import collections\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihs7ZenhZnuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3258db3-4ef9-4d88-b1d4-c78ffe7cecb4"
      },
      "source": [
        "path = '/content/drive/MyDrive/challenges-in-representation-learning-facial-expression-recognition-challenge/'\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example_submission.csv',\n",
              " 'fer2013.tar.gz',\n",
              " 'icml_face_data.csv',\n",
              " 'test.csv',\n",
              " 'train.csv',\n",
              " 'facial_model.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U5ZAovdRmsG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxska9FicTXT"
      },
      "source": [
        "data = pd.read_csv(path+'icml_face_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5TO_DIdcYJI"
      },
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\" Prepare data for modeling \n",
        "        input: data frame with labels und pixel data\n",
        "        output: image and label array \"\"\"\n",
        "    \n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "    \n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "        \n",
        "    return image_array, image_label\n",
        "\n",
        "def plot_examples(label=0):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n",
        "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
        "    axs = axs.ravel()\n",
        "    for i in range(5):\n",
        "        idx = data[data['emotion']==label].index[i]\n",
        "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
        "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
        "        axs[i].set_xticklabels([])\n",
        "        axs[i].set_yticklabels([])\n",
        "\n",
        "def plot_all_emotions():\n",
        "    fig, axs = plt.subplots(1, 7, figsize=(30, 12))\n",
        "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
        "    axs = axs.ravel()\n",
        "    for i in range(7):\n",
        "        idx = data[data['emotion']==i].index[i]\n",
        "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
        "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
        "        axs[i].set_xticklabels([])\n",
        "        axs[i].set_yticklabels([])\n",
        "        \n",
        "def plot_image_and_emotion(test_image_array, test_image_label, pred_test_labels, image_number):\n",
        "    \"\"\" Function to plot the image and compare the prediction results with the label \"\"\"\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
        "    \n",
        "    bar_label = emotions.values()\n",
        "    \n",
        "    axs[0].imshow(test_image_array[image_number], 'gray')\n",
        "    axs[0].set_title(emotions[test_image_label[image_number]])\n",
        "    \n",
        "    axs[1].bar(bar_label, pred_test_labels[image_number], color='orange', alpha=0.7)\n",
        "    axs[1].grid()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_compare_distributions(array1, array2, title1='', title2=''):\n",
        "    df_array1 = pd.DataFrame()\n",
        "    df_array2 = pd.DataFrame()\n",
        "    df_array1['emotion'] = array1.argmax(axis=1)\n",
        "    df_array2['emotion'] = array2.argmax(axis=1)\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
        "    x = emotions.values()\n",
        "    \n",
        "    y = df_array1['emotion'].value_counts()\n",
        "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
        "    for key_missed in keys_missed:\n",
        "        y[key_missed] = 0\n",
        "    axs[0].bar(x, y.sort_index(), color='orange')\n",
        "    axs[0].set_title(title1)\n",
        "    axs[0].grid()\n",
        "    \n",
        "    y = df_array2['emotion'].value_counts()\n",
        "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
        "    for key_missed in keys_missed:\n",
        "        y[key_missed] = 0\n",
        "    axs[1].bar(x, y.sort_index())\n",
        "    axs[1].set_title(title2)\n",
        "    axs[1].grid()\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNaSL97FcfP6"
      },
      "source": [
        "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DS-8GH4cj1i"
      },
      "source": [
        "train_image_array, train_image_label = prepare_data(data[data[' Usage']=='Training'])\n",
        "val_image_array, val_image_label = prepare_data(data[data[' Usage']=='PrivateTest'])\n",
        "test_image_array, test_image_label = prepare_data(data[data[' Usage']=='PublicTest'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_0klnPbcnJN"
      },
      "source": [
        "train_images = train_image_array.reshape((train_image_array.shape[0], 48, 48, 1))\n",
        "train_images = train_images.astype('float32')/255\n",
        "val_images = val_image_array.reshape((val_image_array.shape[0], 48, 48, 1))\n",
        "val_images = val_images.astype('float32')/255\n",
        "test_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\n",
        "test_images = test_images.astype('float32')/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7Dv95nPdZgu"
      },
      "source": [
        "train_labels = to_categorical(train_image_label)\n",
        "val_labels = to_categorical(val_image_label)\n",
        "test_labels = to_categorical(test_image_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYGDvrE6djZq"
      },
      "source": [
        "\n",
        "class_weight = dict(zip(range(0, 7), (((data[data[' Usage']=='Training']['emotion'].value_counts()).sort_index())/len(data[data[' Usage']=='Training']['emotion'])).tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVDfkTDiduGs"
      },
      "source": [
        "import tensorflow.keras as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def preprocess_data(X, Y):\n",
        "    X_p = K.applications.resnet50.preprocess_input(X)\n",
        "    Y_p = K.utils.to_categorial(Y, 7)  # 7 is for 7 emotions\n",
        "    return X_p, Y_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2aIUb_-d0mi"
      },
      "source": [
        "# train_images, train_labels = preprocess_data(train_images, train_labels)\n",
        "# valid_images, valid_labels = preprocess_data(valid_images, valid_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og4_BxFzd4FF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2a7a03-3149-4bc5-ce7a-3027a176ca97"
      },
      "source": [
        "input_t = K.Input(shape=(48,48,3))  # sensitive\n",
        "res_model = K.applications.ResNet50(include_top = False, weights = \"imagenet\", input_tensor = input_t)\n",
        "\n",
        "for layers in res_model.layers[:143]:  # sensitive\n",
        "    layers.trainable = False\n",
        "    \n",
        "for i, layer in enumerate(res_model.layers):\n",
        "    print(i, layer.name, \"-\", layer.trainable)\n",
        "\n",
        "image_size_resnets = (48,48)   #  sensitive, more likely to increase the dimension to get more data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_2 - False\n",
            "1 conv1_pad - False\n",
            "2 conv1_conv - False\n",
            "3 conv1_bn - False\n",
            "4 conv1_relu - False\n",
            "5 pool1_pad - False\n",
            "6 pool1_pool - False\n",
            "7 conv2_block1_1_conv - False\n",
            "8 conv2_block1_1_bn - False\n",
            "9 conv2_block1_1_relu - False\n",
            "10 conv2_block1_2_conv - False\n",
            "11 conv2_block1_2_bn - False\n",
            "12 conv2_block1_2_relu - False\n",
            "13 conv2_block1_0_conv - False\n",
            "14 conv2_block1_3_conv - False\n",
            "15 conv2_block1_0_bn - False\n",
            "16 conv2_block1_3_bn - False\n",
            "17 conv2_block1_add - False\n",
            "18 conv2_block1_out - False\n",
            "19 conv2_block2_1_conv - False\n",
            "20 conv2_block2_1_bn - False\n",
            "21 conv2_block2_1_relu - False\n",
            "22 conv2_block2_2_conv - False\n",
            "23 conv2_block2_2_bn - False\n",
            "24 conv2_block2_2_relu - False\n",
            "25 conv2_block2_3_conv - False\n",
            "26 conv2_block2_3_bn - False\n",
            "27 conv2_block2_add - False\n",
            "28 conv2_block2_out - False\n",
            "29 conv2_block3_1_conv - False\n",
            "30 conv2_block3_1_bn - False\n",
            "31 conv2_block3_1_relu - False\n",
            "32 conv2_block3_2_conv - False\n",
            "33 conv2_block3_2_bn - False\n",
            "34 conv2_block3_2_relu - False\n",
            "35 conv2_block3_3_conv - False\n",
            "36 conv2_block3_3_bn - False\n",
            "37 conv2_block3_add - False\n",
            "38 conv2_block3_out - False\n",
            "39 conv3_block1_1_conv - False\n",
            "40 conv3_block1_1_bn - False\n",
            "41 conv3_block1_1_relu - False\n",
            "42 conv3_block1_2_conv - False\n",
            "43 conv3_block1_2_bn - False\n",
            "44 conv3_block1_2_relu - False\n",
            "45 conv3_block1_0_conv - False\n",
            "46 conv3_block1_3_conv - False\n",
            "47 conv3_block1_0_bn - False\n",
            "48 conv3_block1_3_bn - False\n",
            "49 conv3_block1_add - False\n",
            "50 conv3_block1_out - False\n",
            "51 conv3_block2_1_conv - False\n",
            "52 conv3_block2_1_bn - False\n",
            "53 conv3_block2_1_relu - False\n",
            "54 conv3_block2_2_conv - False\n",
            "55 conv3_block2_2_bn - False\n",
            "56 conv3_block2_2_relu - False\n",
            "57 conv3_block2_3_conv - False\n",
            "58 conv3_block2_3_bn - False\n",
            "59 conv3_block2_add - False\n",
            "60 conv3_block2_out - False\n",
            "61 conv3_block3_1_conv - False\n",
            "62 conv3_block3_1_bn - False\n",
            "63 conv3_block3_1_relu - False\n",
            "64 conv3_block3_2_conv - False\n",
            "65 conv3_block3_2_bn - False\n",
            "66 conv3_block3_2_relu - False\n",
            "67 conv3_block3_3_conv - False\n",
            "68 conv3_block3_3_bn - False\n",
            "69 conv3_block3_add - False\n",
            "70 conv3_block3_out - False\n",
            "71 conv3_block4_1_conv - False\n",
            "72 conv3_block4_1_bn - False\n",
            "73 conv3_block4_1_relu - False\n",
            "74 conv3_block4_2_conv - False\n",
            "75 conv3_block4_2_bn - False\n",
            "76 conv3_block4_2_relu - False\n",
            "77 conv3_block4_3_conv - False\n",
            "78 conv3_block4_3_bn - False\n",
            "79 conv3_block4_add - False\n",
            "80 conv3_block4_out - False\n",
            "81 conv4_block1_1_conv - False\n",
            "82 conv4_block1_1_bn - False\n",
            "83 conv4_block1_1_relu - False\n",
            "84 conv4_block1_2_conv - False\n",
            "85 conv4_block1_2_bn - False\n",
            "86 conv4_block1_2_relu - False\n",
            "87 conv4_block1_0_conv - False\n",
            "88 conv4_block1_3_conv - False\n",
            "89 conv4_block1_0_bn - False\n",
            "90 conv4_block1_3_bn - False\n",
            "91 conv4_block1_add - False\n",
            "92 conv4_block1_out - False\n",
            "93 conv4_block2_1_conv - False\n",
            "94 conv4_block2_1_bn - False\n",
            "95 conv4_block2_1_relu - False\n",
            "96 conv4_block2_2_conv - False\n",
            "97 conv4_block2_2_bn - False\n",
            "98 conv4_block2_2_relu - False\n",
            "99 conv4_block2_3_conv - False\n",
            "100 conv4_block2_3_bn - False\n",
            "101 conv4_block2_add - False\n",
            "102 conv4_block2_out - False\n",
            "103 conv4_block3_1_conv - False\n",
            "104 conv4_block3_1_bn - False\n",
            "105 conv4_block3_1_relu - False\n",
            "106 conv4_block3_2_conv - False\n",
            "107 conv4_block3_2_bn - False\n",
            "108 conv4_block3_2_relu - False\n",
            "109 conv4_block3_3_conv - False\n",
            "110 conv4_block3_3_bn - False\n",
            "111 conv4_block3_add - False\n",
            "112 conv4_block3_out - False\n",
            "113 conv4_block4_1_conv - False\n",
            "114 conv4_block4_1_bn - False\n",
            "115 conv4_block4_1_relu - False\n",
            "116 conv4_block4_2_conv - False\n",
            "117 conv4_block4_2_bn - False\n",
            "118 conv4_block4_2_relu - False\n",
            "119 conv4_block4_3_conv - False\n",
            "120 conv4_block4_3_bn - False\n",
            "121 conv4_block4_add - False\n",
            "122 conv4_block4_out - False\n",
            "123 conv4_block5_1_conv - False\n",
            "124 conv4_block5_1_bn - False\n",
            "125 conv4_block5_1_relu - False\n",
            "126 conv4_block5_2_conv - False\n",
            "127 conv4_block5_2_bn - False\n",
            "128 conv4_block5_2_relu - False\n",
            "129 conv4_block5_3_conv - False\n",
            "130 conv4_block5_3_bn - False\n",
            "131 conv4_block5_add - False\n",
            "132 conv4_block5_out - False\n",
            "133 conv4_block6_1_conv - False\n",
            "134 conv4_block6_1_bn - False\n",
            "135 conv4_block6_1_relu - False\n",
            "136 conv4_block6_2_conv - False\n",
            "137 conv4_block6_2_bn - False\n",
            "138 conv4_block6_2_relu - False\n",
            "139 conv4_block6_3_conv - False\n",
            "140 conv4_block6_3_bn - False\n",
            "141 conv4_block6_add - False\n",
            "142 conv4_block6_out - False\n",
            "143 conv5_block1_1_conv - True\n",
            "144 conv5_block1_1_bn - True\n",
            "145 conv5_block1_1_relu - True\n",
            "146 conv5_block1_2_conv - True\n",
            "147 conv5_block1_2_bn - True\n",
            "148 conv5_block1_2_relu - True\n",
            "149 conv5_block1_0_conv - True\n",
            "150 conv5_block1_3_conv - True\n",
            "151 conv5_block1_0_bn - True\n",
            "152 conv5_block1_3_bn - True\n",
            "153 conv5_block1_add - True\n",
            "154 conv5_block1_out - True\n",
            "155 conv5_block2_1_conv - True\n",
            "156 conv5_block2_1_bn - True\n",
            "157 conv5_block2_1_relu - True\n",
            "158 conv5_block2_2_conv - True\n",
            "159 conv5_block2_2_bn - True\n",
            "160 conv5_block2_2_relu - True\n",
            "161 conv5_block2_3_conv - True\n",
            "162 conv5_block2_3_bn - True\n",
            "163 conv5_block2_add - True\n",
            "164 conv5_block2_out - True\n",
            "165 conv5_block3_1_conv - True\n",
            "166 conv5_block3_1_bn - True\n",
            "167 conv5_block3_1_relu - True\n",
            "168 conv5_block3_2_conv - True\n",
            "169 conv5_block3_2_bn - True\n",
            "170 conv5_block3_2_relu - True\n",
            "171 conv5_block3_3_conv - True\n",
            "172 conv5_block3_3_bn - True\n",
            "173 conv5_block3_add - True\n",
            "174 conv5_block3_out - True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oRjuz8fCUmQ",
        "outputId": "86fb5408-dde5-4606-9724-eb35f1fb1e9b"
      },
      "source": [
        "print(np.shape(train_labels))\n",
        "\n",
        "t_array = np.zeros([np.shape(train_images)[0], 48, 48, 3])\n",
        "\n",
        "t_array[:,:,:,0:1] = train_images\n",
        "t_array[:,:,:,1:2] = train_images\n",
        "t_array[:,:,:,2:3] = train_images\n",
        "\n",
        "v_array = np.zeros([np.shape(val_images)[0],48,48,3])\n",
        "\n",
        "v_array[:,:,:,0:1] = val_images\n",
        "v_array[:,:,:,1:2] = val_images\n",
        "v_array[:,:,:,2:3] = val_images\n",
        "\n",
        "print(np.shape(t_array))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28709, 7)\n",
            "(28709, 48, 48, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrE7oM8Pd-cd"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "model = K.models.Sequential()\n",
        "model.add(K.layers.Lambda(lambda image: tf.image.resize(image, image_size_resnets)))\n",
        "model.add(res_model)\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(256, activation = 'relu'))\n",
        "model.add(K.layers.Dropout(0.5))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(128, activation = 'relu'))\n",
        "model.add(K.layers.Dropout(0.5))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(64, activation = 'relu'))\n",
        "model.add(K.layers.Dropout(0.5))\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dense(7, activation = 'softmax'))\n",
        "\n",
        " \n",
        "check_point = K.callbacks.ModelCheckpoint(filepath = \"resnets_on_emoji.h5\", monitor = \"val_acc\", mode = \"max\", save_best_only = True)\n",
        " \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = K.optimizers.Adam(), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYXX94p8eFX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c46eff-3e9d-496f-e7ed-02acd68e3d64"
      },
      "source": [
        "history = model.fit(t_array, train_labels, batch_size = 32, epochs = 10, verbose = 1,\n",
        "                    validation_data = (v_array, val_labels), callbacks = [check_point])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "898/898 [==============================] - 35s 33ms/step - loss: 1.9975 - accuracy: 0.2093 - val_loss: 1.9062 - val_accuracy: 0.2491\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/10\n",
            "898/898 [==============================] - 28s 32ms/step - loss: 1.7499 - accuracy: 0.2856 - val_loss: 3.1002 - val_accuracy: 0.2775\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/10\n",
            "898/898 [==============================] - 29s 32ms/step - loss: 1.6823 - accuracy: 0.3290 - val_loss: 3.7799 - val_accuracy: 0.1193\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/10\n",
            "898/898 [==============================] - 28s 32ms/step - loss: 1.6700 - accuracy: 0.3338 - val_loss: 4.0706 - val_accuracy: 0.2455\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/10\n",
            "898/898 [==============================] - 29s 33ms/step - loss: 1.6339 - accuracy: 0.3581 - val_loss: 3.2511 - val_accuracy: 0.1293\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/10\n",
            "898/898 [==============================] - 29s 33ms/step - loss: 1.6191 - accuracy: 0.3656 - val_loss: 3.0013 - val_accuracy: 0.2310\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/10\n",
            "898/898 [==============================] - 29s 33ms/step - loss: 1.6078 - accuracy: 0.3746 - val_loss: 1.7579 - val_accuracy: 0.2945\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/10\n",
            "898/898 [==============================] - 29s 32ms/step - loss: 1.6040 - accuracy: 0.3770 - val_loss: 1.8006 - val_accuracy: 0.2249\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/10\n",
            "898/898 [==============================] - 28s 32ms/step - loss: 1.5905 - accuracy: 0.3813 - val_loss: 1.9070 - val_accuracy: 0.2756\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/10\n",
            "898/898 [==============================] - 30s 33ms/step - loss: 1.5832 - accuracy: 0.3858 - val_loss: 1.6583 - val_accuracy: 0.3580\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 48, 48, 3)         0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 2, 2, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8192)              32768     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 7)                 455       \n",
            "=================================================================\n",
            "Total params: 25,761,287\n",
            "Trainable params: 17,132,295\n",
            "Non-trainable params: 8,628,992\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEWQZTq3eP-3"
      },
      "source": [
        "K.learning_phase = K.backend.learning_phase\n",
        "###  removable part...  to check whether it affects the accuracy or not\n",
        "# train_images, train_labels = preprocess_data(train_image, train_labels)\n",
        "# valid_images, valid_labels = preprocess_data(valid_images, valid_labels)\n",
        "###################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = hisory.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label = 'Training_acc')\n",
        "plt.plot(epochs, val_acc, 'bo', label = 'Validation_acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI9fQ9dfeYYv"
      },
      "source": [
        "plt.plot(epochs, loss, 'r', label = \"Training_loss\")\n",
        "plt.plot(epochs, val_loss, 'ro', label = \"Validation_loss\")\n",
        "plt.tilte('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}